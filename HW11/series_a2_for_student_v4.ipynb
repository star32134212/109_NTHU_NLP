{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alg_to_t import estimate_tef_and_tfe\n",
    "import kenlm\n",
    "import csv\n",
    "from math import isclose, exp, log10\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the language model ...\n",
      "You can use variable \"lm\" to score a sentence.\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "preparing the data ...\n",
      "Here are examples of the data.\n",
      "(['i', \"'ve\", 'bought', 'a', 'car', '.'], ['我', '買', '了', '輛', '汽車', '。'])\n",
      "(['she', \"'s\", 'got', 'a', 'boyfriend', '.'], ['她', '交', '了', '個', '男朋友', '。'])\n",
      "(['there', 'was', 'a', 'sudden', 'loud', 'noise', '.'], ['突然', '發出', '一', '聲', '巨響', '。'])\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "You can use variables \"t_ef\" denoting t(e|f) and \"t_fe\" denoting t(f|e).\n"
     ]
    }
   ],
   "source": [
    "print('loading the language model ...')\n",
    "lm = kenlm.Model('./bnc.bin')\n",
    "print('You can use variable \"lm\" to score a sentence.')\n",
    "print('\\n' + '=' * 100 + '\\n')\n",
    "\n",
    "\n",
    "parallel_tokens_path = './cambridge_fast_align_corpus.txt'\n",
    "parallel_tokens = []\n",
    "print('preparing the data ...')\n",
    "with open(parallel_tokens_path, 'r', encoding='utf8') as f:\n",
    "    parallel_corpus = f.read().split('\\n')\n",
    "    for sent_pair in parallel_corpus:\n",
    "        en_sent, ch_sent = sent_pair.split(' ||| ')\n",
    "        en_tokens = en_sent.split(' ')\n",
    "        ch_tokens = ch_sent.split(' ')\n",
    "        parallel_tokens.append((en_tokens, ch_tokens))\n",
    "\n",
    "print('Here are examples of the data.')\n",
    "for x in parallel_tokens[:3]:\n",
    "    print(x)\n",
    "print('\\n' + '=' * 100 + '\\n')\n",
    "\n",
    "\n",
    "with open('./cambridge_sym_en-ch.align', 'r') as f:\n",
    "    en_ch_alignments = f.read().split('\\n')[:-1]\n",
    "\n",
    "\n",
    "# print('estimating t(e|f) and t(f|e) ...')\n",
    "# t_ef, t_fe = estimate_tef_and_tfe(parallel_tokens, en_ch_alignments)\n",
    "t_ef = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "t_fe = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "p_g_ef = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "p_g_fe = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "with open('word_table.csv', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        w_e = row['ephrase']\n",
    "        w_c = row['cphrase']\n",
    "        t_ef[w_e][w_c] = float(row['pec'])\n",
    "        t_fe[w_c][w_e] = float(row['pce'])\n",
    "        p_g_fe[w_c][w_e] = row['pec']\n",
    "        p_g_ef[w_e][w_c] = row['pce']\n",
    "print('You can use variables \"t_ef\" denoting t(e|f) and \"t_fe\" denoting t(f|e).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conditional_prob(token_ch, translation_dict):\n",
    "    prob_dict = {}\n",
    "    for token_en, sub_dict in translation_dict.items():\n",
    "        if token_ch in sub_dict:\n",
    "            prob_dict[token_en] = sub_dict[token_ch]\n",
    "    return prob_dict\n",
    "\n",
    "def check_top_k_prob(token_ch, k, translation_dict):\n",
    "    prob_dict = get_conditional_prob(token_ch, translation_dict)\n",
    "    prob_list = list(prob_dict.items())\n",
    "    prob_sum = sum(prob_dict.values())\n",
    "    prob_list.sort(key=lambda x:x[1], reverse=True)\n",
    "\n",
    "    if not isclose(prob_sum, 1.):\n",
    "        print(f\"Warning. sum of p( e | {token_ch}) = {prob_sum}, not close to 1.\")\n",
    "\n",
    "    for token_en, prob in prob_list[:k]:\n",
    "        print(f\"p({token_en} | {token_ch}) = {prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(regret | 遺憾) = 0.3256876796844739\n",
      "p(sorry | 遺憾) = 0.16972485822707165\n",
      "p(regrettable | 遺憾) = 0.15596337329270898\n",
      "p(regrets | 遺憾) = 0.10091743355525822\n",
      "p(regrettably | 遺憾) = 0.05045871677762911\n",
      "p(regretful | 遺憾) = 0.05045871677762911\n",
      "p(unfortunately | 遺憾) = 0.041284454803088066\n",
      "p(disappointed | 遺憾) = 0.013761484934362687\n",
      "p(pity | 遺憾) = 0.013761484934362687\n",
      "p(regretted | 遺憾) = 0.013761484934362687\n"
     ]
    }
   ],
   "source": [
    "check_top_k_prob('遺憾', 10, t_ef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# level 1: paraphrasing with word-based translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paraphrasing by controlling word sense\n",
    "我們在作業說明中有介紹到，一種 control word sense 的 paraphrasing 方法。而我們的資料是 parallel corpus，所以可以使用這個方法。\n",
    "接下來的 level A 就會一步一步地讓你們把這個方法實作出來。  \n",
    "首先我們需要根據給定的 $e_1$, $e_2$ 以及它們之間的 anchor $f$，使用 translation probability distribution $t(e|f)$ and $t(f|e)$ 算出 $e1$ 與 $e_2$ 的 paraphrase probability。  \n",
    "然後我們就能算出 $e_1$ 與其他所有不是 $e_1$ 的 english word 的 paraphrase probability，並取最高的前 5 名。  \n",
    "接著我們需要用 language model 來衡量句子把 $e_1$ 替換成候選人之後的 score，score 越高代表越合適。但這邊的 score 算出來是負的，需要先取 exponential。然後將 score 乘上 paraphrase probability，並將候選人按照乘積由高到低重新排序。  \n",
    "最後我們就能用前面實作的各個 function 將 paraphrasing 實作出來。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute paraphrase probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'suggest', 'this', 'method']\n",
      "['我', '建議', '這個', '方法']\n",
      "1 suggest\n",
      "1 建議\n",
      "proposed\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "example_en_tokens = [\"I\", \"suggest\", \"this\", \"method\"]\n",
    "example_ch_tokens = [\"我\", \"建議\", \"這個\", \"方法\"]\n",
    "example_e1_idx = 1\n",
    "example_e1 = example_en_tokens[example_e1_idx]\n",
    "example_f_idx = 1\n",
    "example_f = example_ch_tokens[example_f_idx]\n",
    "example_e2 = 'proposed'\n",
    "\n",
    "print(example_en_tokens)\n",
    "print(example_ch_tokens)\n",
    "print(example_e1_idx, example_e1)\n",
    "print(example_f_idx, example_f)\n",
    "print(example_e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'suggest', 'this', 'method']\n",
      "['我', '建議', '這個', '方法']\n",
      "\n",
      "before paraphrasing: I ```suggest``` this method\n",
      "after paraphrasing: I ```proposed``` this method\n"
     ]
    }
   ],
   "source": [
    "test_idx = 1\n",
    "e1_idx = 1\n",
    "en_tokens, ch_tokens = example_en_tokens, example_ch_tokens\n",
    "print(en_tokens, ch_tokens, sep=\"\\n\", end=\"\\n\\n\")\n",
    "\n",
    "test_paraphrasing_half(example_en_tokens, example_ch_tokens, e1_idx, example_f_idx, t_ef, t_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_paraphrase_prob(e1: str,\n",
    "                            e2: str,\n",
    "                            f: str,\n",
    "                            t_ef: defaultdict,\n",
    "                            t_fe: defaultdict):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        e1: 待替換的單字\n",
    "        e2: 用來替換的單字\n",
    "        f: e1 與 e2 替換用的 anchor，e1 => f => e2\n",
    "        t_ef: t(e|f)\n",
    "        t_fe: t(f|e)\n",
    "\n",
    "    Returns:\n",
    "        prob: e1 => f => e2 的 paraphrase probability\n",
    "    \"\"\"\n",
    "    prob = 0.\n",
    "    #### YOUR CODE HERE ####\n",
    "    \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22497118206479705"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" result\n",
    ".22497\n",
    "\"\"\"\n",
    "compute_paraphrase_prob(example_e1, example_e2, example_f, t_ef, t_fe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect candidates for paraphrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_5_paraphrases(e1: str,\n",
    "                            f: str,\n",
    "                            t_ef: defaultdict,\n",
    "                            t_fe: defaultdict):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        e1: 待替換的單字\n",
    "        f: anchor\n",
    "        t_ef: t(e|f)\n",
    "        t_fe: t(f|e)\n",
    "\n",
    "    Returns:\n",
    "        candidates: paraphrase probability 前 5 高的其他英文單字，以及它們對應的\n",
    "        paraphrase probability。\n",
    "\n",
    "        example:\n",
    "        [(c1, paraphrase_prob1),\n",
    "         ... ,\n",
    "         (c5, paraphrase_prob5)]\n",
    "    \"\"\"\n",
    "    candidates = []#[('', 0.) for i in range(5)]\n",
    "    #### YOUR CODE HERE ####\n",
    "    \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('proposed', 0.22497118206479705),\n",
       " ('proposals', 0.1329054475991522),\n",
       " ('proposal', 0.11513612557416811),\n",
       " ('recommendations', 0.07940394719871861),\n",
       " ('propose', 0.045689473608494055)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" result\n",
    "[('proposed', 0.22497118206479705),\n",
    " ('proposals', 0.1329054475991522),\n",
    " ('proposal', 0.11513612557416811),\n",
    " ('recommendations', 0.07940394719871861),\n",
    " ('propose', 0.045689473608494055)]\n",
    "\"\"\"\n",
    "get_first_5_paraphrases(example_e1, example_f, t_ef ,t_fe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-rank candidates for paraphrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_paraphrases(en_tokens: List[str],\n",
    "                       e1_idx: int,\n",
    "                       candidates: List):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        en_tokens: 英文句子的 tokens (words)\n",
    "        e1_idx: 待替換的單字在 en_tokens 中的 index\n",
    "        candidates: get_first_5_paraphrases 的 output\n",
    "\n",
    "    Returns:\n",
    "        candidates_rerank: [c1, c2, ..., c5]，按照它們的 paraphrase probability 與\n",
    "        exponential of sentence probability 的乘積排序，由高到低。\n",
    "    \"\"\"\n",
    "    candidates_rerank = []\n",
    "    #### YOUR CODE HERE ####\n",
    "    \n",
    "    \n",
    "    return candidates_rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['proposed', 'propose', 'proposals', 'proposal', 'recommendations']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" result\n",
    "['proposed', 'propose', 'proposals', 'proposal', 'recommendations']\n",
    "\"\"\"\n",
    "\n",
    "example_candidates = get_first_5_paraphrases(example_e1, example_f, t_ef ,t_fe)\n",
    "rerank_paraphrases(example_en_tokens, example_e1_idx, example_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paraphrasing given a ch_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrasing(en_tokens: List[str],\n",
    "                 ch_tokens: List[str],\n",
    "                 e1_idx: int,\n",
    "                 f_idx: int,\n",
    "                 t_ef: defaultdict,\n",
    "                 t_fe: defaultdict):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        en_tokens: 英文句子的 tokens (words)\n",
    "        ch_tokens: 對應的中文句子的 tokens (斷詞)\n",
    "        e1_idx: 待替換的單字在 en_tokens 中的 index\n",
    "        f_idx: 與 e1 align 的中文斷詞在 ch_tokens 中的 index\n",
    "        t_ef: t(e|f)\n",
    "        t_fe: t(f|e)\n",
    "\n",
    "    Returns:\n",
    "        e2: rerank 之後第一名的 candidate\n",
    "    \"\"\"\n",
    "    e2 = ''\n",
    "    #### YOUR CODE HERE ####\n",
    "    \n",
    "\n",
    "    return e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'proposed'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" result\n",
    "'proposed'\n",
    "\"\"\"\n",
    "\n",
    "paraphrasing(example_en_tokens, example_ch_tokens, example_e1_idx, example_f_idx, t_ef, t_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_paraphrasing_half(en_tokens, ch_tokens, e1_idx, f_idx, t_ef, t_fe):\n",
    "    e2 = paraphrasing(en_tokens, ch_tokens, e1_idx, f_idx, t_ef, t_fe)\n",
    "    print('before paraphrasing:', highlight_word(en_tokens, e1_idx))\n",
    "    print('after paraphrasing:', highlight_word(en_tokens[:e1_idx] + [e2] + en_tokens[(e1_idx+1):],\n",
    "                                                e1_idx))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_paraphrasing(en_tokens, ch_tokens, e1_idx, alg_of_sent, t_ef, t_fe):\n",
    "    idx_map = defaultdict(lambda: [])\n",
    "    for en_ch_alg in alg_of_sent.split(' '):\n",
    "        en_idx, ch_idx = list(map(lambda x: int(x), en_ch_alg.split('-')))\n",
    "        idx_map[en_idx].append(ch_idx)\n",
    "    f_idx = idx_map[e1_idx][0]\n",
    "    e2 = paraphrasing(en_tokens, ch_tokens, e1_idx, f_idx, t_ef, t_fe)\n",
    "    print('before paraphrasing:', highlight_word(en_tokens, e1_idx))\n",
    "    print('after paraphrasing:', highlight_word(en_tokens[:e1_idx] + [e2] + en_tokens[(e1_idx+1):],\n",
    "                                                e1_idx))\n",
    "    return\n",
    "\n",
    "\n",
    "def highlight_word(en_tokens, highlight_idx):\n",
    "    word = f'```{en_tokens[highlight_idx]}```'\n",
    "    sent = ' '.join(en_tokens[:highlight_idx] + [word] + en_tokens[(highlight_idx+1):])\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shall', 'i', 'put', 'on', 'a', 'dvd', 'to', 'amuse', 'the', 'kids', '?']\n",
      "['要不要', '給', '孩子', '放段', '影片', '，', '逗', '他們', '開心', '？']\n",
      "\n",
      "before paraphrasing: shall i put on a dvd to amuse the ```kids``` ?\n",
      "after paraphrasing: shall i put on a dvd to amuse the ```children``` ?\n"
     ]
    }
   ],
   "source": [
    "test_idx = 2001\n",
    "e1_idx = 9\n",
    "en_tokens, ch_tokens = copy.deepcopy(parallel_tokens[test_idx])\n",
    "print(en_tokens, ch_tokens, sep=\"\\n\", end=\"\\n\\n\")\n",
    "\n",
    "test_paraphrasing(en_tokens, ch_tokens, e1_idx, en_ch_alignments[test_idx], t_ef, t_fe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General paraphrasing\n",
    "有時候我們想做 paraphrasing 的英文句子不一定會有像 parallel corpus 一樣的對應 foreign sentence，所以更 general 的做法應該是考慮所有的 foreign word 與 English word 的 parapharse probability。  \n",
    "關於這個作法的介紹已經寫在作業說明當中，請自行參考它並實作下面給出的 function。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_e1_to_e2_prob_dict(e1, t_ef, t_fe):\n",
    "    # update dict p such that p[e2] = t(e2 | e1) \n",
    "    # NOTE : p[e1] = t(e1 | e1) should NOT be zero\n",
    "    p = defaultdict(lambda: 0.0) \n",
    "    #### CODE HERE ####\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_paraphrasing(en_tokens, e1_idx, t_ef, t_fe):\n",
    "    e2 = ''\n",
    "    #### YOUR CODE HERE ####\n",
    "    \n",
    "    return e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_general_paraphrasing(en_tokens, e1_idx, t_ef, t_fe):\n",
    "    e2 = general_paraphrasing(en_tokens, e1_idx, t_ef, t_fe)\n",
    "    print('before paraphrasing:', highlight_word(en_tokens, e1_idx))\n",
    "    print('after paraphrasing:', highlight_word(en_tokens[:e1_idx] + [e2] + en_tokens[(e1_idx+1):],\n",
    "                                                e1_idx))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'suggest', 'this', 'method'] 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'proposed'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(example_en_tokens, example_e1_idx)\n",
    "general_paraphrasing(example_en_tokens, example_e1_idx, t_ef, t_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'suggest', 'this', 'method']\n",
      "proposed\n",
      "\n",
      "before paraphrasing: I ```suggest``` this method\n",
      "after paraphrasing: I ```proposed``` this method\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "print(example_en_tokens)\n",
    "print(general_paraphrasing(example_en_tokens, example_e1_idx, t_ef, t_fe), end='\\n\\n')\n",
    "\n",
    "test_general_paraphrasing(example_en_tokens[:], example_e1_idx, t_ef, t_fe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding with noisy channel model\n",
    "利用以上程式，對整句話進行Paraphrasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParaphrasingModel():\n",
    "    def __init__(self, t_ef, t_fe, prob_function):\n",
    "        self.t_ef = t_ef\n",
    "        self.t_fe = t_fe\n",
    "        self.p_func = prob_function\n",
    "        self.phrase_set = set(t_ef.keys())\n",
    "        \n",
    "    def __getitem__(self, phrase_in):\n",
    "        # phase_in should be a tuple of words e.g. (word,)\n",
    "        assert(type(phrase_in) == tuple)\n",
    "        if len(phrase_in) == 1 and phrase_in not in self.phrase_set: # unknown word, OOV\n",
    "            p = defaultdict(lambda: 0.0)\n",
    "            p[phrase_in] = 1.\n",
    "            return p\n",
    "        \n",
    "        return self.p_func(phrase_in, self.t_ef, self.t_fe)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys of t_ef and t_fe are words. We hope that the keys are tuples.\n",
    "tt_ef = defaultdict(lambda:defaultdict(lambda:0.0))\n",
    "for e in t_ef:\n",
    "    for f in t_ef[e]:\n",
    "        tt_ef[(e,)][(f,)] = t_ef[e][f]\n",
    "\n",
    "tt_fe = defaultdict(lambda:defaultdict(lambda:0.0))\n",
    "for f in t_fe:\n",
    "    for e in t_fe[f]:\n",
    "        tt_fe[(f,)][(e,)] = t_fe[f][e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = ParaphrasingModel(tt_ef, tt_fe, get_e1_to_e2_prob_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def noisy_channel_decode(sent_in, beam_size, tm, lm, verbose = False, bos=True):\n",
    "    # The following code implements a monotone decoding\n",
    "    # algorithm (one that doesn't permute the target phrases).\n",
    "    # Hence all hypotheses in stacks[i] represent translations of \n",
    "    # the first i words of the input sentence. You should generalize\n",
    "    # this so that they can represent translations of *any* i words.\n",
    "    hypothesis = namedtuple(\"hypothesis\", \"logprob, lm_state, predecessor, phrase\")\n",
    "    state = kenlm.State()\n",
    "    if bos:\n",
    "        lm.BeginSentenceWrite(state) #Use <s> as context.  If you don't want <s>, use lm.NullContextWrite(state).\n",
    "    else:\n",
    "        lm.NullContextWrite(state)\n",
    "    initial_hypothesis = hypothesis(0.0, state, None, None)\n",
    "    # stacks = [{} for _ in sent_in] + [{}]\n",
    "    # stacks[0][state] = initial_hypothesis\n",
    "    stacks = [[] for _ in sent_in] + [[]]\n",
    "    stacks[0].append(initial_hypothesis)\n",
    "    for ii, stack in enumerate(stacks[:-1]):\n",
    "    #     for h in sorted(stack.values(),key=lambda h: -h.logprob)[:beam_size]: # prune\n",
    "        print(\"\\nnow\", ii)\n",
    "        for h in sorted(stack,key=lambda h: -h.logprob)[:beam_size]: # prune\n",
    "            for j in range(ii+1,len(sent_in)+1):\n",
    "                print(h.phrase, ii, j, end='\\t')\n",
    "    #             print(sent_in[ii:j])\n",
    "                if sent_in[ii:j] in tm.phrase_set:\n",
    "    #                 for phrase_new, trans_prob in tm[sent_in[i:j]].items():\n",
    "                    for phrase_new, trans_prob in sorted(tm[sent_in[ii:j]].items(), key=lambda x:-x[1])[:2*beam_size]:\n",
    "                        # print(phrase_new)\n",
    "                        logprob = h.logprob + log10(trans_prob)\n",
    "                        lm_state = h.lm_state\n",
    "                        for word in phrase_new:\n",
    "                            lm_state_new = kenlm.State()\n",
    "                            word_logprob = lm.BaseScore(lm_state, word, lm_state_new)\n",
    "                            lm_state = lm_state_new\n",
    "                            logprob += word_logprob\n",
    "                        logprob += lm.BaseScore(lm_state, \"</s>\", kenlm.State()) if j == len(sent_in) else 0.0\n",
    "                        new_hypothesis = hypothesis(logprob, lm_state, h, phrase_new)\n",
    "    #                     if lm_state not in stacks[j] or stacks[j][lm_state].logprob < logprob: # second case is recombination\n",
    "    #                         stacks[j][lm_state] = new_hypothesis\n",
    "                        stacks[j].append(new_hypothesis)\n",
    "    # winner = max(stacks[-1].values(), key=lambda h: h.logprob)\n",
    "    print(\"\\n\\nRESULT:\")\n",
    "    result = []\n",
    "    for winner in sorted(stacks[-1],key=lambda h: -h.logprob)[:beam_size]:\n",
    "        def extract_english(h): \n",
    "            return \"\" if h.predecessor is None else \"%s%s \" % (extract_english(h.predecessor), \" \".join(h.phrase))\n",
    "        sent_out = extract_english(winner)\n",
    "        print(sent_out)\n",
    "        result.append(sent_out)\n",
    "\n",
    "        if verbose:\n",
    "    #         def extract_tm_logprob(h):\n",
    "    #             return 0.0 if h.predecessor is None else h.phrase.logprob + extract_tm_logprob(h.predecessor)\n",
    "    #         tm_logprob = extract_tm_logprob(winner)\n",
    "            lm_score = print(model.score(sent_out, bos = True, eos = True))\n",
    "            sys.stderr.write(\"LM = %f, TM = %f, Total = %f\\n\" % \n",
    "              (winner.logprob - tm_logprob, tm_logprob, winner.logprob))\n",
    "    #           (winner.logprob - tm_logprob, tm_logprob, winner.logprob))\n",
    "        print()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I suggest this method\n",
      "\n",
      "now 0\n",
      "None 0 1\tNone 0 2\tNone 0 3\tNone 0 4\t\n",
      "now 1\n",
      "('i',) 1 2\t('i',) 1 3\t('i',) 1 4\t('my',) 1 2\t('my',) 1 3\t('my',) 1 4\t('in',) 1 2\t('in',) 1 3\t('in',) 1 4\t('me',) 1 2\t('me',) 1 3\t('me',) 1 4\t('we',) 1 2\t('we',) 1 3\t('we',) 1 4\t\n",
      "now 2\n",
      "('proposals',) 2 3\t('proposals',) 2 4\t('proposal',) 2 3\t('proposal',) 2 4\t('proposed',) 2 3\t('proposed',) 2 4\t('suggestion',) 2 3\t('suggestion',) 2 4\t('suggestions',) 2 3\t('suggestions',) 2 4\t\n",
      "now 3\n",
      "('the',) 3 4\t('the',) 3 4\t('that',) 3 4\t('the',) 3 4\t('that',) 3 4\t\n",
      "\n",
      "RESULT:\n",
      "my suggestion that way \n",
      "\n",
      "my proposal that way \n",
      "\n",
      "my proposed the way \n",
      "\n",
      "my proposal the way \n",
      "\n",
      "my proposals the way \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" RESULT:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "beam_size = 5\n",
    "sent_in = tuple(example_en_tokens)\n",
    "print(\" \".join(example_en_tokens))\n",
    "sent_list = noisy_channel_decode((\"i\", \"suggest\", \"this\", \"method\"), beam_size, tm, lm, bos = False)\n",
    "# sent_list = noisy_channel_decode((), beam_size, tm, lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# level 2 : paraphrasing with phrase-based translation\n",
    "TODO:\n",
    "1. 使用中英句子對```parallel_tokens```和 word alignment ```en_ch_alignments```  \n",
    "   以consistent block的概念計算phrase table，  \n",
    "   也就是phrase英翻中和中翻英的機率:  \n",
    "   t(phrase_ch | phrase_en), t(phrase_en | phrase_ch)  \n",
    "   \n",
    "2. 計算以phrase的Paraphrasing的機率 t(phrase_en_2 | phrase_en_1)  \n",
    "3. 利用函式noisy_channel_decode查看phrase-based的Paraphrasing的效果  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['i', \"'ve\", 'bought', 'a', 'car', '.'], ['我', '買', '了', '輛', '汽車', '。'])\n",
      "0-0 1-0 2-1 2-2 3-3 4-4 5-5\n"
     ]
    }
   ],
   "source": [
    "print(parallel_tokens[0])\n",
    "print(en_ch_alignments[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import consistent_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair(align_str):\n",
    "    return [tuple(int(idx)for idx in a.split(\"-\")) for a in align_str.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (1, 0), (1, 1), (2, 2), (3, 3), (4, 4), (5, 5)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pair(en_ch_alignments[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65418, 65418)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parallel_tokens), len(en_ch_alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CODE HERE ####\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
