{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages and initialize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "import csv\n",
    "import tqdm\n",
    "from copy import deepcopy\n",
    "from math import log, isclose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the tokenized parallel corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_tokens_path = './cambridge_parallel_tokens.tsv'\n",
    "parallel_tokens = []\n",
    "with open(parallel_tokens_path, 'r', encoding='utf8') as f:\n",
    "    for en_tokens, ch_tokens in csv.reader(f, delimiter='\\t'):\n",
    "        parallel_tokens.append([en_tokens.split('#sep#'),\n",
    "                                ch_tokens.split('#sep#')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in parallel_tokens[:3]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Model 1\n",
    "According to the equations in the assignment instructions, if we want to update the translation probablity of a english word $e$ given a foreign word $f$, we have to count how often $e$ aligned with $f$ in the whole parallel corpus. Lets implement the function to count that in a pair of sentences.\n",
    "$$c(e|f;\\mathbf E, \\mathbf F) = \\frac{t(e|f)}{\\sum_{i=0}^{l_F}t(e|f_i)}\\sum_{j=1}^{l_E}\\delta(e, e_j)\\sum_{i=0}^{l_F}\\delta(f, f_i) \\\\\n",
    " \\delta(x, y) = \\begin{array}{lr}1,\\quad if \\quad x == y \\\\ 0, \\quad else \\end{array}$$\n",
    "Note that the word at index 0 of foreign sentence $\\mathbf F$ is NULL token.  \n",
    "In the next two steps, you will implement this count function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Compute the normalization term of the count function\n",
    "In the first step, the returned variable of your function, ```total_e```, should represent $\\sum_{i=0}^{l_F}t(e|f_i)$.  \n",
    "Note that ```total_e``` is the denominator of $c$.  \n",
    "Here is the example of the usage of it :  \n",
    "```total_e['apple']```$= \\sum_{i=0}^{l_F}t(apple|f_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_normalization_term(en_token_list: List[str], ch_token_list: List[str],\n",
    "                               t_ef: defaultdict) -> defaultdict:\n",
    "    total_e = defaultdict(lambda: 0)\n",
    "    #### YOUR CODE HERE ####\n",
    "\n",
    "    return total_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_ch = ['[NULL]', '我', '愛', '你', '愛', '我']\n",
    "example_en = ['i', 'love', 'that', 'you', 'love', 'me']\n",
    "\n",
    "\n",
    "\"\"\" result\n",
    "('[NULL]', 0.6)\n",
    "('我', 1.2)\n",
    "('愛', 1.2)\n",
    "('你', 0.6)\n",
    "\"\"\"\n",
    "for x in compute_normalization_term(example_en, example_ch,\n",
    "                                    defaultdict(lambda: defaultdict(lambda: 0.1))).items():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Collect the count from the given bilingual sentences\n",
    "In the second step, the returned variable of your function, ```count_of_sent```, should represent the complete count function.  \n",
    "  Here is the example of the usage of it :  \n",
    "  ```count_of_sent['apple']['蘋果']``` $= c(apple|蘋果;\\mathbf E, \\mathbf F)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_ef_count(en_token_list: List[str], ch_token_list: List[str],\n",
    "                     t_ef: defaultdict, total_e: defaultdict) -> defaultdict:\n",
    "    count_of_sent = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "    #### YOUR CODE HERE ####\n",
    "\n",
    "    return count_of_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_ch, example_en)\n",
    "print()\n",
    "\n",
    "\"\"\"result\n",
    "('i', '[NULL]', 0.01)\n",
    "('i', '我', 0.02)\n",
    "('i', '愛', 0.02)\n",
    "('i', '你', 0.01)\n",
    "('love', '[NULL]', 0.02)\n",
    "('love', '我', 0.04)\n",
    "('love', '愛', 0.04)\n",
    "('love', '你', 0.02)\n",
    "('that', '[NULL]', 0.01)\n",
    "('that', '我', 0.02)\n",
    "('that', '愛', 0.02)\n",
    "('that', '你', 0.01)\n",
    "('you', '[NULL]', 0.01)\n",
    "('you', '我', 0.02)\n",
    "('you', '愛', 0.02)\n",
    "('you', '你', 0.01)\n",
    "('me', '[NULL]', 0.01)\n",
    "('me', '我', 0.02)\n",
    "('me', '愛', 0.02)\n",
    "('me', '你', 0.01)\n",
    "\"\"\"\n",
    "for x, d in collect_ef_count(example_en, example_ch, \n",
    "                             defaultdict(lambda: defaultdict(lambda: 0.1)),\n",
    "                             defaultdict(lambda: 10.0)).items():\n",
    "    for y, z in d.items():\n",
    "        print((x, y, z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Implement IBM Model 1\n",
    "Now you should be able to re-estimate the lexical probability distribution by\n",
    "$$t(e|f) = \\frac{\\sum_{(\\mathbf E, \\mathbf F)}c(e|f;\\mathbf E, \\mathbf F)}{\\sum_{e'}\\sum_{(\\mathbf e, \\mathbf f)}c(e'|f;\\mathbf E, \\mathbf F)}$$\n",
    "Note: You need to fill the code into where the comment strings start with '####'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ibm_model_1(parallel_corpus_list: List, n_iters: int) -> defaultdict:\n",
    "    en_vocab = set(en_token.lower() for en_token_list, _ in parallel_corpus_list\n",
    "                            for en_token in en_token_list)\n",
    "    ch_vocab = set(ch_token for _, ch_token_list in parallel_corpus_list\n",
    "                            for ch_token in ch_token_list)\n",
    "    print(f'length of en_vocab: {len(en_vocab)}')\n",
    "    print(f'lenght of ch_vocab: {len(ch_vocab)}')\n",
    "    \n",
    "    null_token = '[NULL]'\n",
    "    # Add the Null token\n",
    "    ch_vocab.add(null_token)\n",
    "    \n",
    "    # ptotal_edistribution is uniform.\n",
    "    init_prob = 1 / len(en_vocab)\n",
    "    # t_ef[e][f] means t(e|f)\n",
    "    t_ef = defaultdict(lambda: defaultdict(lambda: init_prob))\n",
    "    \n",
    "    \n",
    "    # EM process\n",
    "    for i in tqdm.tqdm(range(n_iters)):\n",
    "        # initialize\n",
    "        # total_ef_count[e][f] is the numerator ot new t(e|f)\n",
    "        total_ef_count = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "        # total_f[f] is the Denominator ot new t(e|f)\n",
    "        total_f = defaultdict(lambda: 0.0) \n",
    "        token_pair_set = set()\n",
    "        perp = 0\n",
    "        # collect the evidence from corpus\n",
    "        for en_token_list, ch_token_list in parallel_corpus_list:\n",
    "            ch_token_list = [null_token] + ch_token_list\n",
    "            perp += compute_perp(en_token_list, ch_token_list, t_ef)\n",
    "            \n",
    "            total_e = compute_normalization_term(en_token_list, ch_token_list, t_ef)\n",
    "            ef_count_of_sent = collect_ef_count(en_token_list, ch_token_list, t_ef, total_e)\n",
    "            \n",
    "            cur_token_pair_set = set((e, f) for e in en_token_list\n",
    "                                            for f in ch_token_list)\n",
    "            for e, f in cur_token_pair_set:\n",
    "                #### You should write two lines of code here.  ####\n",
    "                #### Update total_ef_count and total_f with ef_count_of_sent ####\n",
    "\n",
    "            token_pair_set.update(cur_token_pair_set)\n",
    "            \n",
    "        print(f'perplexity: {round(perp, 2)}')\n",
    "        # estimate probabilities\n",
    "        t_ef = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        for e, f in token_pair_set:\n",
    "            #### You should write a line of code here. Update t_ef ####\n",
    "        \n",
    "    return t_ef\n",
    "\n",
    "def compute_perp(en_token_list, ch_token_list, t_ef):\n",
    "    l_e = len(en_token_list)\n",
    "    l_f = len(ch_token_list)\n",
    "    p_EF = 0.\n",
    "    for e in en_token_list:\n",
    "        s = 0.\n",
    "        for f in ch_token_list:\n",
    "            s += t_ef[e][f]\n",
    "        p_EF += -log(s)\n",
    "    p_EF += log(l_f**l_e)\n",
    "    return p_EF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "**You have to run the following cells.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ef = ibm_model_1(parallel_tokens, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation\n",
    "**You have to run the following cells.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conditional_prob(token_ch, translation_dict):\n",
    "    prob_dict = {}\n",
    "    for token_en, sub_dict in translation_dict.items():\n",
    "        if token_ch in sub_dict:\n",
    "            prob_dict[token_en] = sub_dict[token_ch]\n",
    "    return prob_dict\n",
    "\n",
    "def check_top_k_prob(token_ch, k, translation_dict):\n",
    "    prob_dict = get_conditional_prob(token_ch, translation_dict)\n",
    "    prob_list = list(prob_dict.items())\n",
    "    prob_sum = sum(prob_dict.values())\n",
    "    prob_list.sort(key=lambda x:x[1], reverse=True)\n",
    "    \n",
    "    if not isclose(prob_sum, 1.):\n",
    "        print(f\"Warning. sum of p( e | {token_ch}) = {prob_sum}, not close to 1.\")\n",
    "    \n",
    "    for token_en, prob in prob_list[:k]:\n",
    "        print(f\"p({token_en} | {token_ch}) = {prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" results for reference\n",
    "p(school | 學校) = 0.7592438820372703\n",
    "p(schools | 學校) = 0.152040212014728\n",
    "p(at | 學校) = 0.08116603608007476\n",
    "p(the | 學校) = 0.0024247348456308397\n",
    "p(has | 學校) = 0.0024026143118947804\n",
    "\"\"\"\n",
    "\n",
    "check_top_k_prob('學校', 5, t_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" results for reference\n",
    "p(businesses | 企業) = 0.46758578752806945\n",
    "p(business | 企業) = 0.3901655681518314\n",
    "p(by | 企業) = 0.03300134949576897\n",
    "p(run | 企業) = 0.02862452153472591\n",
    "p(a | 企業) = 0.019347178868170722\n",
    "\"\"\"\n",
    "\n",
    "check_top_k_prob('企業', 5, t_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" results for reference\n",
    "p(budget | 預算) = 0.675000585629638\n",
    "p(are | 預算) = 0.1145673270578127\n",
    "p(within | 預算) = 0.03939258531542436\n",
    "p(already | 預算) = 0.03737763058988209\n",
    "p(overspent | 預算) = 0.024970159365872654\n",
    "\"\"\"\n",
    "\n",
    "check_top_k_prob('預算', 5, t_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_top_k_prob('世紀', 5, t_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_top_k_prob('紡織', 5, t_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_top_k_prob('校長', 5, t_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_top_k_prob('美國', 5, t_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_top_k_prob('員警', 5, t_ef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Model 2\n",
    "Recall that the count functions of lexical translation and alignment in the assignment instructions:\n",
    "$$\\begin{eqnarray}\n",
    "c_1(e|f;\\mathbf E, \\mathbf F) \n",
    "&=&\\sum_{j=1}^{l_E}\\sum_{i=0}^{l_F}\n",
    "\\frac{t(e|f)a(i|j,l_E,l_F )\\delta(e, e_j)\\delta(f, f_i)}\n",
    "{\\sum_{i'=0}^{l_F}t(e|f_{i'})a(i'|j,l_E,l_F )}\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "c_2(i|j, l_E, l_F;\\mathbf E, \\mathbf F) &=& \n",
    "\\frac{t(e_j|f_i) a(i|j,l_E,l_F)}{\\sum_{i'=0}^{l_F}t(e_j|f_{i'})a(i'|j,l_E,l_F )}\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "Similarly, you will implement the count functions in two steps. In the final step, you would be asked to complete the implementation of IBM Model 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Compute the normalization term of the count function\n",
    "```total_e[english_word][j]``` is a defaultdict with two keys, and it is actually the count function of alignment.  \n",
    "You may be aware to the fact that $\\sum_{j}$```total_e[english_word][j]``` is the count function of lexical translation if you note the difference between the normalization terms of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_alg_normalization_term(en_token_list: List[str], ch_token_list: List[str],\n",
    "                                   t_ef: defaultdict, q_alg: defaultdict) -> defaultdict:\n",
    "    total_e = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    l_e = len(en_token_list)\n",
    "    # the lenth of foreign sentence does not take NULL token into account\n",
    "    l_f = len(ch_token_list) - 1\n",
    "    \n",
    "    for j in range(1, l_e + 1):\n",
    "        en_word = en_token_list[j - 1]\n",
    "        for i in range(l_f + 1):\n",
    "            ch_word = ch_token_list[i]\n",
    "            #### YOUR CODE HERE ####\n",
    "\n",
    "    return total_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_ch, example_en)\n",
    "print()\n",
    "\n",
    "\"\"\" result\n",
    "('i', 1, 0.12000000000000002)\n",
    "('love', 2, 0.12000000000000002)\n",
    "('love', 5, 0.12000000000000002)\n",
    "('that', 3, 0.12000000000000002)\n",
    "('you', 4, 0.12000000000000002)\n",
    "('me', 6, 0.12000000000000002)\n",
    "\"\"\"\n",
    "for x, d in compute_alg_normalization_term(example_en, example_ch,\n",
    "                                           defaultdict(lambda: defaultdict(lambda: 0.1)),\n",
    "                                           defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0.2))))\n",
    "                                          ).items():\n",
    "    for y, z in d.items():\n",
    "        print((x, y, z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Collect the count for translation and the count for alignment from the given bilingual sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_count(en_token_list: List[str], ch_token_list: List[str],\n",
    "                    t_ef: defaultdict, q_alg: defaultdict,\n",
    "                    total_e: defaultdict) -> defaultdict:\n",
    "    ef_count = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    alg_count = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0.0))))\n",
    "    l_e = len(en_token_list)\n",
    "    # the lenth of foreign sentence does not take NULL token into account\n",
    "    l_f = len(ch_token_list) - 1\n",
    "    \n",
    "    for j in range(1, l_e + 1):\n",
    "        en_word = en_token_list[j - 1]\n",
    "        for i in range(l_f + 1):\n",
    "            ch_word = ch_token_list[i]\n",
    "            #### YOUR CODE HERE ####\n",
    "            \n",
    "            \n",
    "    return (ef_count, alg_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_ch, example_en)\n",
    "print()\n",
    "\n",
    "\"\"\"result\n",
    "('i', '[NULL]', 0.20000000000000004)\n",
    "('i', '我', 0.4000000000000001)\n",
    "('i', '愛', 0.4000000000000001)\n",
    "('i', '你', 0.20000000000000004)\n",
    "('love', '[NULL]', 0.4000000000000001)\n",
    "('love', '我', 0.8000000000000002)\n",
    "('love', '愛', 0.8000000000000002)\n",
    "('love', '你', 0.4000000000000001)\n",
    "('that', '[NULL]', 0.20000000000000004)\n",
    "('that', '我', 0.4000000000000001)\n",
    "('that', '愛', 0.4000000000000001)\n",
    "('that', '你', 0.20000000000000004)\n",
    "('you', '[NULL]', 0.20000000000000004)\n",
    "('you', '我', 0.4000000000000001)\n",
    "('you', '愛', 0.4000000000000001)\n",
    "('you', '你', 0.20000000000000004)\n",
    "('me', '[NULL]', 0.20000000000000004)\n",
    "('me', '我', 0.4000000000000001)\n",
    "('me', '愛', 0.4000000000000001)\n",
    "('me', '你', 0.20000000000000004)\n",
    "\n",
    "(0, 1, 0.20000000000000004)\n",
    "...\n",
    "(5, 5, 0.20000000000000004)\n",
    "(5, 6, 0.20000000000000004)\n",
    "\"\"\"\n",
    "example_ef, example_alg = collect_count(example_en, example_ch,\n",
    "                                        defaultdict(lambda: defaultdict(lambda: 0.1)),\n",
    "                                        defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0.2)))),\n",
    "                                        defaultdict(lambda: defaultdict(lambda: 0.1))\n",
    "                                       )\n",
    "for x, d in example_ef.items():\n",
    "    for y, z in d.items():\n",
    "        print((x, y, z))\n",
    "print()\n",
    "\n",
    "for x, d in example_alg.items():\n",
    "    for y, z in d.items():\n",
    "        print((x, y, z[len(example_en)][len(example_ch)-1]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3. Implement IBM Model 2\n",
    "Now you should be able to re-estimate the probability distribution of lexical translation and that of alignment by  \n",
    "$$t(e|f) = \\frac{\\sum_{(\\mathbf E, \\mathbf F)}c_1(e|f;\\mathbf E, \\mathbf F)}{\\sum_{e'}\\sum_{(\\mathbf E, \\mathbf F)}c_1(e'|f;\\mathbf E, \\mathbf F)}$$\n",
    "\n",
    "$$a(i|j, l_E, l_F) = \\frac{\\sum_{(\\mathbf E, \\mathbf F)}c_2(i|j, l_E, l_F;\\mathbf E, \\mathbf F)}{\\sum_{i'}\\sum_{(\\mathbf E, \\mathbf F)}c_2(i'|j, l_E, l_F;\\mathbf E, \\mathbf F)}$$\n",
    "\n",
    "Note: You need to fill the code into where the comment strings start with '####'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ibm_model_2(parallel_corpus_list: List, n_iters: int,\n",
    "                init_t_ef: defaultdict) -> Tuple[defaultdict, defaultdict]:\n",
    "    en_vocab = set(en_token.lower() for en_token_list, _ in parallel_corpus_list\n",
    "                            for en_token in en_token_list)\n",
    "    ch_vocab = set(ch_token for _, ch_token_list in parallel_corpus_list\n",
    "                            for ch_token in ch_token_list)\n",
    "    print(f'length of en_vocab: {len(en_vocab)}')\n",
    "    print(f'lenght of ch_vocab: {len(ch_vocab)}')\n",
    "    null_token = '[NULL]'\n",
    "    # Add the Null token\n",
    "    ch_vocab.add(null_token)\n",
    "    \n",
    "    t_ef = deepcopy(init_t_ef)\n",
    "    q_alg = get_initial_q_alg(parallel_corpus_list) # q_alg[i][j][l_e][l_f] means a(i | j, l_e, l_f)\n",
    "    \n",
    "    for i in tqdm.tqdm(range(n_iters)):\n",
    "        total_ef_count = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "        total_f = defaultdict(lambda: 0.0)\n",
    "\n",
    "        # total_alg_count[i][j][l_e][l_f] is the numerator of new a(i | j, l_e, l_f)\n",
    "        total_alg_count = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0.0))))\n",
    "        # total_alg_count[j][l_e][l_f] is the denominator of new a(i | j, l_e, l_f)\n",
    "        total_alg = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0.0)))\n",
    "\n",
    "        token_pair_set = set()\n",
    "        perp = 0\n",
    "        # collect the evidence from corpus\n",
    "        for en_token_list, ch_token_list in parallel_corpus_list:\n",
    "            ch_token_list = [null_token] + ch_token_list\n",
    "            perp += compute_perp(en_token_list, ch_token_list, t_ef, q_alg)\n",
    "            # the lenth of foreign sentence does not take NULL token into account\n",
    "            l_f = len(ch_token_list) - 1\n",
    "            l_e = len(en_token_list)\n",
    "\n",
    "            total_e = compute_alg_normalization_term(en_token_list, ch_token_list, t_ef, q_alg)\n",
    "            ef_count_of_sent, alg_count_of_sent = collect_count(en_token_list, ch_token_list,\n",
    "                                                                t_ef, q_alg, total_e)\n",
    "            \n",
    "            cur_token_pair_set = set((e, f) for e in en_token_list\n",
    "                                            for f in ch_token_list)\n",
    "            token_pair_set.update(cur_token_pair_set)\n",
    "            for e, f in cur_token_pair_set:\n",
    "                #### You should write two lines of code here.  ####\n",
    "                #### Update total_ef_count and total_f with ef_count_of_sent####\n",
    "                \n",
    "                \n",
    "            # collect counts\n",
    "            for j in range(1, l_e+1):\n",
    "                for i in range(l_f+1):\n",
    "                    #### You should write two lines of code here.  ####\n",
    "                    #### Update total_alg_count, total_alg with alg_count_of_sent ####\n",
    "                    \n",
    "                    \n",
    "        print(f'perplexity: {round(perp, 2)}')\n",
    "        \n",
    "        # Estimate the new lexical translation probabilities\n",
    "        t_ef = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        for e, f in token_pair_set:\n",
    "            #### You should write a line of code here. Update t_ef ####\n",
    "            \n",
    "            \n",
    "        # Estimate the new alignment probabilities\n",
    "        q_alg = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0.0))))\n",
    "        for en_token_list, ch_token_list in parallel_corpus_list:\n",
    "            l_f = len(ch_token_list)\n",
    "            l_e = len(en_token_list)\n",
    "            for i in range(l_f + 1):\n",
    "                for j in range(1, l_e + 1):\n",
    "                    #### You should write a line of code here. Update q_alg ####\n",
    "                    \n",
    "    return (t_ef, q_alg)\n",
    "\n",
    "def get_initial_q_alg(parallel_corpus_list: List) -> defaultdict:\n",
    "    q_alg = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0.0))))\n",
    "    \n",
    "    for en_token_list, ch_token_list in parallel_corpus_list:\n",
    "        l_e = len(en_token_list)\n",
    "        l_f = len(ch_token_list)\n",
    "        q_initial = 1 / (l_f + 1)\n",
    "        for i in range(0, l_f + 1): # 0 for NULL token\n",
    "            for j in range(1, l_e + 1):\n",
    "                q_alg[i][j][l_e][l_f] = q_initial\n",
    "    return q_alg\n",
    "\n",
    "def compute_perp(en_token_list, ch_token_list, t_ef, q_alg):\n",
    "    perp = 0\n",
    "    l_e = len(en_token_list)\n",
    "    l_f = len(ch_token_list) - 1\n",
    "    for j in range(1, l_e+1):\n",
    "        en_word = en_token_list[j - 1]\n",
    "        cur_perp = 0\n",
    "        for i in range(l_f + 1):\n",
    "            ch_word = ch_token_list[i]\n",
    "            cur_perp += t_ef[en_word][ch_word] * q_alg[i][j][l_e][l_f]\n",
    "        perp += -log(cur_perp)\n",
    "    return perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation\n",
    "**You have to run the following cells.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ef_2, q_alg = ibm_model_2(parallel_tokens, 15, t_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_prob(token_ch, k, translation_dict):\n",
    "    prob_dict = get_conditional_prob(token_ch, translation_dict)\n",
    "    prob_list = list(prob_dict.items())\n",
    "    prob_sum = sum(prob_dict.values())\n",
    "    prob_list.sort(key=lambda x:x[1], reverse=True)\n",
    "    if not isclose(prob_sum, 1.):\n",
    "        print(f\"Warning. sum of p( e | {token_ch}) = {prob_sum}, not close to 1.\")\n",
    "    \n",
    "    return prob_list[:k]\n",
    "\n",
    "def translate(ch_token_list: List[str], l_e: int, translation_dict, align):\n",
    "    l_f = len(ch_token_list)\n",
    "    null_token = '[NULL]'\n",
    "    ch_token_list.insert(0, null_token)\n",
    "    k = 3\n",
    "    \n",
    "    en_candidates = []\n",
    "    for token_ch in ch_token_list:\n",
    "        en_candidates.append(get_top_k_prob(token_ch, 10, translation_dict))\n",
    "\n",
    "    for j in range(1, l_e+1):\n",
    "        possible_list = []\n",
    "        for i, ens in enumerate(en_candidates):\n",
    "            for token_en, prob in ens:\n",
    "                possible_list.append((token_en, align[i][j][l_e][l_f] * prob))\n",
    "        possible_list.sort(key=lambda x:x[1], reverse=True)\n",
    "        print(f\"possible candidates for the {j}-th English word:\")\n",
    "        for token_en, prob in possible_list[:k]:\n",
    "            print(f\"{token_en}\\t\\tprobability:{prob}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" result\n",
    "possible candidates for the 1-th word:\n",
    "i\t\tprobability:0.3051610136893576\n",
    "a\t\tprobability:0.08537920692347092\n",
    "(\t\tprobability:0.07314717156010928\n",
    "\n",
    "possible candidates for the 2-th word:\n",
    "i\t\tprobability:0.2220627843510443\n",
    "bike\t\tprobability:0.1721509922781537\n",
    "bicycle\t\tprobability:0.07748032610499858\n",
    "\n",
    "possible candidates for the 3-th word:\n",
    "riding\t\tprobability:0.08936528673455782\n",
    "bike\t\tprobability:0.08840401330382662\n",
    "will\t\tprobability:0.07680236346367117\n",
    "\n",
    "possible candidates for the 4-th word:\n",
    "i\t\tprobability:0.15463795029280586\n",
    "riding\t\tprobability:0.08968372901915127\n",
    "will\t\tprobability:0.07672989741828276\n",
    "\n",
    "possible candidates for the 5-th word:\n",
    "bike\t\tprobability:0.3455995109696529\n",
    "bicycle\t\tprobability:0.15554463240264935\n",
    "i\t\tprobability:0.08811550005889636\n",
    "\"\"\"\n",
    "translate(['我', '會', '騎', '腳踏車'], 5, t_ef_2, q_alg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(['今天','天氣','很','好'], 5, t_ef_2, q_alg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
